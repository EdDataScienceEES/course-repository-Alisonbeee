# Tutorial topic with questions and answers created by Alison Stewart

#### Set readings 
* [Paper 1:](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0200303)

Fraser, H., Parker, T., Nakagawa, S., Barnett, A. and Fidler, F., 2018. Questionable research practices in ecology and evolution. PloS one, 13(7), p.e0200303.

* [Paper 2:](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=+The+garden+of+forking+paths%3A+Why+multiple+comparisons+can+be+a+problem%2C+even+when+there+is+no+“fishing+expedition”+or+“p-hacking”+and+the+research++hypothesis+was+posited+ahead+of+time&btnG=)

Gelman, A. and Loken, E., 2013. The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. Department of Statistics, Columbia University.



# Questionable research practices 

The P-value is I would argue the most treasured thing in science, at least from an undergrads opinion. The feeling of excitement, to have your hypothesis proven correct is amazing.

 What is not so amazing is the weight it carries in publications, without a significant result you’re not likely to be published and with careers and money on the line some will do anything to get the magical significant result. 

The anything they plan to do is known as questionable research practices , we will go into a more detailed description of them in the first question but simply they are way to get the results you need by fudging the results or analysis. As we gain more of an understanding to why? We will hopefully learn ways to combat Mal practice in our own future work.

Something that was published yesterday in the news related to this topic. Was In the current corona virus vaccine race the  AstraZenica vaccine study has been getting some additional scrutiny over potential p-hacking and breaking from their preregistration. 

This this really goes to show that this is still happening and prevalent in science

(That's because different doses of the vaccine were mistakenly used in the trial. Some volunteers were given shots half the planned strength, in error. Yet that "wrong" dose turned out to be a winner.

But the sample size was a third of the main analysis so you can’t a definite result yet.)


So moving on with our intro we are going to discuss the first paper

# Paper 1:

Time to start with the first paper titled Questionable research practices in ecology and evolution. 
 
It explores the results of a large survey of researchers use of questionable research practices (QRP). 
Then understands the reasons and cause of QRP in ecology and evolutionary sciences.

Question 1:
What where the 3 main questionable research practices the paper chose to explore and briefly describe them.


* P-hacking
refers to a set of activities: checking the statistical significance of results before deciding whether to collect more data; stopping data collection early because results reached statistical significance; deciding whether to exclude data points (e.g., outliers) only after checking the impact on statistical significance and not reporting the impact of the data exclusion; adjusting statistical models, for instance by including or excluding covariates based on the resulting strength of the main effect of interest; and rounding of a p value to meet a statistical significance threshold 

* Cherry picking 
includes failing to report dependent or response variables or relationships that did not reach statistical significance or other threshold and/or failing to report conditions or treatments that did not reach statistical significance or other threshold. 

* HARKing (Hypothesising After Results are Known)
includes presenting ad hoc and/ or unexpected findings as though they had been predicted all along; and presenting exploratory work as though it was confirmatory hypothesis testing.

Sub question:
Now we’ve just heard all the definitions does anyone think they have done any of the three questionable research practices, either on purpose or innocently without realising it was wrong? 

I can start off with me that, I have definitely cherry picked in the past, I would say mainly because I didn’t understand the extra statistical tests or just didn’t realise I should also present non-significant results.

Why did you do that?

Or how did you know never to do that?


Question 2:
Why do you think that statically significant results are preferred in journals? Do you agree with this preference?

Scientists have big incentives to get significant results to continue their funding 

A interesting result will also be preferred and this is where I believe HARKing occurs making it seem like this hypothesised  result just came to them in an ecological vision, when really it was found by chance during the study. 


I disagree with the need for significant results needed for publication because it totally wrecks the balance between real results and false positives, in the video I sent out, it had a nice visual analogy, where when the majority of the null results are not published it leaves a third being false positives, which is a huge increase from only 5% being incorrectly true.


Question 3:
In the methods they reclassified the disciplinary status of each survey and added extra data before analysis. Why did they do this? Do you believe this was good practice or not?

I thought this was a very interesting thing to do on a paper stating bad practices, because though I don’t think there is anything wrong with realising you need to add more data before you start, but the fact they make note to preregistrations as a solution but they obviously had not put much thought into data size before they commenced. 

I felt the reclassification should have not have came up in the first place and that they should have required an answer to what type of researcher they where. It seems like an error was made and this was their somewhat genius solution to solve it though I will give them that


Question 4:
What was the highest reported questionable practice? Did this or any of the results surprise you?

Cherry picking was most common
* Cherry picking 
includes failing to report dependent or response variables or relationships that did not reach statistical significance or other threshold and/or failing to report conditions or treatments that did not reach statistical significance or other threshold. 

Sub Question
Was this the easiest to get away with? Or maybe the least frowned upon?


Going on from that conversation we are going to go onto paper 2


# Paper 2:

Titled: The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research 
hypothesis was posited ahead of time∗ 
 is not directly environmental but follows up from the last paper with more an understanding point of view and includes some interesting real examples.

Question 1:
The introduction suggests that the terms p-hacking and fishing are misleading. Do you agree with this statement? 

I thought this part was really interesting because it could have been taken a few ways, that they really do believe that these terms are perceived harsher than they should be but also that maybe this start is to be polite and avoid anger from the papers they are calling out in their study. 

Question 2:
In the third example about menstrual cycle and vote intentions, what where the main problems with this published study? Did the paper analysis contain QRP?

First off I just loved how absurd this example was that you would actually change how you vote because of your stage in the menstrual cycle.

I had a look at the paper too and it said they are all from the school of business or management so I’m not sure what lead them to taking on the scientific study 

1. The study was a one off survey so no change in their political opinion could be examined over the ovulation period
2. The study at hand thus has very low power (even more so given the inexact measurement of fertility based on a survey response), hence any observed difference is likely attributable to noise 
3. They did not note all of the possible interactions and just simplified aspects such as just married and single

Yes it contained cherry picking of what interaction 
It was not wrote here but I feel they may have p-hacked by only having a sample that was not properly surveyed.

Question 3: 
What can be done to avoid QRP in future studies?

Preregistration - defining the entire data-collection and data-analysis protocol ahead of time. 

Prepublication replication - The idea is to perform two experiments, the first being exploratory but still theory-based, and the second being purely confirmatory with its own preregistered protocol. 

Analysis all relevant comparisons, not just focusing on whatever happens to be statistically significant. - We have elsewhere argued that multilevel modelling can resolve multiple-comparisons issues 



Question 4:
Do you think it is possible to completely avoid all questionable research practices in the future? Why is that the case or not?

Do I think it is possible yes, will it happen, no, not with the current status quo with the way papers are published currently.


Final sub question:
How do you all feel joining a scientific world filled with bad practices and unreproducible experiments? Maybe worried or keen to change the status quo?

Id say I am a little worried that not just maybe my own papers won’t be published due to lack of significant results, but also in the knowledge it is so prevalent, now its a lot harder to know what paper to trust


To wrap up Include:
Conclusion to discussion
Key areas for future research

I hope you all enjoyed this topic as much as I did!

The topic of questionable research practices is much bigger than once presumed as these papers suggest. That the lowly scientist isn’t truly at fault, either from ignorance or pressure from publishers, who are arguably the large drivers for the prevalence of questionable research practices. 

Though we found they can be avoided from pre registration, pre publication replication or better multilevel modelling. And I believe that all of these are accessible to any scientist who seeks to avoid questionable research practices in the future going on. 

I would like to state that my aim was not to suggest abandoning the p-value but to realise it can be manipulated 

I really like how the last paper ended with the line “Criticism is easy, doing research is hard. Flaws can be found in any research design if you look hard enough.” 

Basically it is impossible to prevent all p hacking but we can definitely decrease their instances



